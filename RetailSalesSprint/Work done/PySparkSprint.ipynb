{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aZF3hsVSpZDK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Sales Data Analysis\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyScala For 5000 Sales Record**"
      ],
      "metadata": {
        "id": "NJG4DMAl8H-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/sample_data/5000 Sales Records.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "dtb8N7xUqAV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f4a992-eb9c-46db-fef0-78a31ab350ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|      Item Type|\n",
            "+---------------+\n",
            "|      Baby Food|\n",
            "|         Snacks|\n",
            "|      Beverages|\n",
            "|         Cereal|\n",
            "|         Snacks|\n",
            "|  Personal Care|\n",
            "|        Clothes|\n",
            "|        Clothes|\n",
            "|        Clothes|\n",
            "|      Baby Food|\n",
            "|Office Supplies|\n",
            "|      Cosmetics|\n",
            "|         Cereal|\n",
            "|      Cosmetics|\n",
            "|      Baby Food|\n",
            "|           Meat|\n",
            "|         Cereal|\n",
            "|Office Supplies|\n",
            "|      Beverages|\n",
            "|      Baby Food|\n",
            "+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Display the number of sales yearwise.**"
      ],
      "metadata": {
        "id": "0qjL_JMR3_PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, to_date, col\n",
        "\n",
        "df = df.withColumn(\"Order_Date\", to_date(col(\"Order Date\"), \"M/d/yyyy\"))\n",
        "df.show(5)\n",
        "\n",
        "df = df.withColumn(\"Year\", year(col(\"Order_Date\")))\n",
        "df.show(5)\n",
        "\n",
        "yearly_sales = df.groupBy(\"Year\").count().orderBy(\"Year\")\n",
        "yearly_sales = yearly_sales.withColumnRenamed(\"count\", \"Yearly Sales\")\n",
        "yearly_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_jUzaFU3MEa",
        "outputId": "43ddc5a9-43ce-4fdc-eaee-ca82b2044179"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "|              Region|             Country|Item Type|Sales Channel|Order Priority|Order Date| Order ID|Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|Order_Date|Year|\n",
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "|Central America a...|Antigua and Barbuda |Baby Food|       Online|             M|12/20/2013|957081544|1/11/2014|       552|    255.28|   159.42|    140914.56|  87999.84|    52914.72|2013-12-20|2013|\n",
            "|Central America a...|              Panama|   Snacks|      Offline|             C|  7/5/2010|301644504|7/26/2010|      2167|    152.58|    97.44|    330640.86| 211152.48|   119488.38|2010-07-05|2010|\n",
            "|              Europe|      Czech Republic|Beverages|      Offline|             C| 9/12/2011|478051030|9/29/2011|      4778|     47.45|    31.79|     226716.1| 151892.62|    74823.48|2011-09-12|2011|\n",
            "|                Asia|         North Korea|   Cereal|      Offline|             L| 5/13/2010|892599952|6/15/2010|      9016|     205.7|   117.11|    1854591.2|1055863.76|   798727.44|2010-05-13|2010|\n",
            "|                Asia|           Sri Lanka|   Snacks|      Offline|             C| 7/20/2015|571902596|7/27/2015|      7542|    152.58|    97.44|   1150758.36| 734892.48|   415865.88|2015-07-20|2015|\n",
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "|              Region|             Country|Item Type|Sales Channel|Order Priority|Order Date| Order ID|Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|Order_Date|Year|\n",
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "|Central America a...|Antigua and Barbuda |Baby Food|       Online|             M|12/20/2013|957081544|1/11/2014|       552|    255.28|   159.42|    140914.56|  87999.84|    52914.72|2013-12-20|2013|\n",
            "|Central America a...|              Panama|   Snacks|      Offline|             C|  7/5/2010|301644504|7/26/2010|      2167|    152.58|    97.44|    330640.86| 211152.48|   119488.38|2010-07-05|2010|\n",
            "|              Europe|      Czech Republic|Beverages|      Offline|             C| 9/12/2011|478051030|9/29/2011|      4778|     47.45|    31.79|     226716.1| 151892.62|    74823.48|2011-09-12|2011|\n",
            "|                Asia|         North Korea|   Cereal|      Offline|             L| 5/13/2010|892599952|6/15/2010|      9016|     205.7|   117.11|    1854591.2|1055863.76|   798727.44|2010-05-13|2010|\n",
            "|                Asia|           Sri Lanka|   Snacks|      Offline|             C| 7/20/2015|571902596|7/27/2015|      7542|    152.58|    97.44|   1150758.36| 734892.48|   415865.88|2015-07-20|2015|\n",
            "+--------------------+--------------------+---------+-------------+--------------+----------+---------+---------+----------+----------+---------+-------------+----------+------------+----------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----+------------+\n",
            "|Year|Yearly Sales|\n",
            "+----+------------+\n",
            "|2010|         632|\n",
            "|2011|         658|\n",
            "|2012|         678|\n",
            "|2013|         660|\n",
            "|2014|         660|\n",
            "|2015|         679|\n",
            "|2016|         670|\n",
            "|2017|         363|\n",
            "+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Display the number of orders for each item**"
      ],
      "metadata": {
        "id": "18stIH6I4qLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_sales = df.groupBy(\"Item Type\").count().orderBy(\"Item Type\")\n",
        "item_sales = item_sales.withColumnRenamed(\"count\", \"Total Sales\")\n",
        "item_sales.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M991YJL4zHD",
        "outputId": "45e8b1ea-42a0-4b4d-dca7-d3850292b4b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+\n",
            "|      Item Type|Total Sales|\n",
            "+---------------+-----------+\n",
            "|      Baby Food|        445|\n",
            "|      Beverages|        447|\n",
            "|         Cereal|        385|\n",
            "|        Clothes|        386|\n",
            "|      Cosmetics|        424|\n",
            "|         Fruits|        447|\n",
            "|      Household|        424|\n",
            "|           Meat|        399|\n",
            "|Office Supplies|        420|\n",
            "|  Personal Care|        415|\n",
            "|         Snacks|        398|\n",
            "|     Vegetables|        410|\n",
            "+---------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Display the country with highest sale**"
      ],
      "metadata": {
        "id": "HQuhj6U45UFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum, desc\n",
        "\n",
        "country_wise_sales = df.groupBy(\"Country\").sum(\"Total Revenue\")\n",
        "country_wise_sales = country_wise_sales.orderBy(desc(\"sum(Total Revenue)\"))\n",
        "top_country = country_wise_sales.limit(1)\n",
        "top_country.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-bZTEKD5TZn",
        "outputId": "ee3a0c33-f2ed-4bd0-df26-60bfaccbd013"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+\n",
            "|Country| sum(Total Revenue)|\n",
            "+-------+-------------------+\n",
            "| Rwanda|6.039873958999999E7|\n",
            "+-------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark For retailsales data**"
      ],
      "metadata": {
        "id": "SCxWfZe0uUwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/sample_data/retailsales.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "DOoFP_8eqfZU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, month, year, sum as _sum, to_date,udf,lower\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def calculate_fiscal_year(country, year, month):\n",
        "    country=country.strip().lower()\n",
        "    if country == \"india\" and month < 4:\n",
        "        return year - 1\n",
        "    elif country == \"usa\" and month < 10:\n",
        "        return year - 1\n",
        "    else:\n",
        "        return year\n",
        "\n",
        "fiscal_udf = udf(calculate_fiscal_year, IntegerType())\n",
        "\n",
        "\n",
        "df_with_date_parts = df.withColumn(\"DateParsed\", to_date(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Year\", year(col(\"DateParsed\")))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Month\", month(col(\"DateParsed\")))\n",
        "\n",
        "df_with_fiscal = df_with_date_parts.withColumn(\"FiscalYear\", fiscal_udf(col(\"Country\"), col(\"Year\"), col(\"Month\")))\n",
        "\n",
        "df_with_fiscal.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zewtZ6y783Bm",
        "outputId": "dc34268d-06f1-447b-8d22-a1dc24237287"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------+-------+----------+----+-----+----------+\n",
            "|Sales_Date|Amount|Country|Product|DateParsed|Year|Month|FiscalYear|\n",
            "+----------+------+-------+-------+----------+----+-----+----------+\n",
            "|01/23/2005|350000|India  |bear   |2005-01-23|2005|1    |2004      |\n",
            "|01/27/2005|380000|India  |visky  |2005-01-27|2005|1    |2004      |\n",
            "|02/12/2005|450000|India  |Rum    |2005-02-12|2005|2    |2004      |\n",
            "|01/23/2006|500000|USA    |bear   |2006-01-23|2006|1    |2005      |\n",
            "|01/27/2006|550000|USA    |rum    |2006-01-27|2006|1    |2005      |\n",
            "|02/12/2006|650000|USA    |Visky  |2006-02-12|2006|2    |2005      |\n",
            "|01/23/2006|500000|China  |Beer   |2006-01-23|2006|1    |2006      |\n",
            "|01/27/2006|550000|China  |Visky  |2006-01-27|2006|1    |2006      |\n",
            "|02/12/2006|658000|China  |Rum    |2006-02-12|2006|2    |2006      |\n",
            "+----------+------+-------+-------+----------+----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.yearly sales report**"
      ],
      "metadata": {
        "id": "gDAEVGogvrxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "yearly_sales_report = df_with_fiscal.groupBy(\"FiscalYear\") \\\n",
        "    .sum(\"Amount\") \\\n",
        "    .withColumnRenamed(\"sum(Amount)\", \"Total_Sales\") \\\n",
        "    .orderBy(\"FiscalYear\")\n",
        "\n",
        "\n",
        "yearly_sales_report.show(truncate=False)\n",
        "yearly_sales_report.write.csv(\"/content/sample_data/yearly_sales_report.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks61RDQX-Vr6",
        "outputId": "93cd1116-4ec2-4979-a1f4-6a654d9054b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|FiscalYear|Total_Sales|\n",
            "+----------+-----------+\n",
            "|2004      |1180000    |\n",
            "|2005      |1700000    |\n",
            "|2006      |1708000    |\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Yearly sum for all count**"
      ],
      "metadata": {
        "id": "1Qa3EVqNv0ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_country_sales = df_with_fiscal.groupBy(\"FiscalYear\", \"Country\") \\\n",
        "    .sum(\"Amount\") \\\n",
        "    .withColumnRenamed(\"sum(Amount)\", \"Total_Sales\") \\\n",
        "    .orderBy(\"FiscalYear\", \"Country\")\n",
        "\n",
        "\n",
        "yearly_country_sales.show(truncate=False)\n",
        "yearly_country_sales.write.csv(\"/content/sample_data/yearly_country_sales.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBvwAwli-mF4",
        "outputId": "f8719266-5695-4ab6-c8b8-13cb1a123cdc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-----------+\n",
            "|FiscalYear|Country|Total_Sales|\n",
            "+----------+-------+-----------+\n",
            "|2004      |India  |1180000    |\n",
            "|2005      |USA    |1700000    |\n",
            "|2006      |China  |1708000    |\n",
            "+----------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Yearly sum for specified country**"
      ],
      "metadata": {
        "id": "xqHk_LpEv_J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specified_country = \"India\"\n",
        "\n",
        "yearly_sales_specific_country = df_with_fiscal.filter(col(\"Country\") == specified_country) \\\n",
        "    .groupBy(\"FiscalYear\") \\\n",
        "    .sum(\"Amount\") \\\n",
        "    .withColumnRenamed(\"sum(Amount)\", \"Total_Sales\") \\\n",
        "    .orderBy(\"FiscalYear\")\n",
        "\n",
        "yearly_sales_specific_country.show(truncate=False)\n",
        "yearly_sales_specific_country.write.csv(\"/content/sample_data/yearly_sales_specific_country.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aX1CxBI-mCa",
        "outputId": "749db625-c277-4f33-de37-ebdbb5517dac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|FiscalYear|Total_Sales|\n",
            "+----------+-----------+\n",
            "|2004      |1180000    |\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. yearly report dumping in the yr_sales_rep. Meanwhile we can apply all aggregation function here.**\n",
        "\n"
      ],
      "metadata": {
        "id": "-x--enOEwF4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, avg, max, min\n",
        "\n",
        "\n",
        "yr_sales_rep = df_with_fiscal.groupBy(\"FiscalYear\").agg(\n",
        "    sum(\"Amount\").alias(\"Total_Sales\"),\n",
        "    avg(\"Amount\").alias(\"Avg_Sales\"),\n",
        "    max(\"Amount\").alias(\"Max_Sales\"),\n",
        "    min(\"Amount\").alias(\"Min_Sales\"),\n",
        ")\n",
        "\n",
        "\n",
        "yr_sales_rep=yr_sales_rep.orderBy(\"FiscalYear\")\n",
        "yr_sales_rep.show(truncate=False)\n",
        "yr_sales_rep.write.csv(\"/content/sample_data/yr_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj_36t_j_vEx",
        "outputId": "35a3764c-92ad-458f-e82c-5a3008db6ac9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-----------------+---------+---------+\n",
            "|FiscalYear|Total_Sales|Avg_Sales        |Max_Sales|Min_Sales|\n",
            "+----------+-----------+-----------------+---------+---------+\n",
            "|2004      |1180000    |393333.3333333333|450000   |350000   |\n",
            "|2005      |1700000    |566666.6666666666|650000   |500000   |\n",
            "|2006      |1708000    |569333.3333333334|658000   |500000   |\n",
            "+----------+-----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.monthly sales report of a perticular year Dumping the data into mn_sales_rep**"
      ],
      "metadata": {
        "id": "WEhldQmWwQb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "filtered_df = df_with_fiscal.filter(col(\"FiscalYear\") == 2006)\n",
        "\n",
        "mn_sales_rep = filtered_df.groupBy(\"Month\").agg(\n",
        "    sum(\"Amount\").alias(\"Monthly_Sales\")\n",
        ")\n",
        "\n",
        "mn_sales_rep = mn_sales_rep.orderBy(\"Month\")\n",
        "\n",
        "mn_sales_rep.show(truncate=False)\n",
        "\n",
        "mn_sales_rep.write.csv(\"/content/sample_data/mn_sales_rep.csv\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56TZvprX_vBT",
        "outputId": "b0e8fcbc-cea6-428e-ad20-f02be63298e7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|Month|Monthly_Sales|\n",
            "+-----+-------------+\n",
            "|1    |1050000      |\n",
            "|2    |658000       |\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.For each quartetr report for each year**"
      ],
      "metadata": {
        "id": "Dn4S8QiMJL7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, sum, col\n",
        "\n",
        "year_to_filter = 2006\n",
        "df_year = df_with_fiscal.filter(col(\"FiscalYear\") == year_to_filter)\n",
        "\n",
        "df_quartered = df_year.withColumn(\"Quarter\",\n",
        "    when(col(\"Month\").between(1, 3), \"Q1\")\n",
        "    .when(col(\"Month\").between(4, 6), \"Q2\")\n",
        "    .when(col(\"Month\").between(7, 9), \"Q3\")\n",
        "    .when(col(\"Month\").between(10, 12), \"Q4\")\n",
        ")\n",
        "\n",
        "quarterly_sales_rep = df_quartered.groupBy(\"Quarter\").agg(\n",
        "    sum(\"Amount\").alias(\"Quarterly_Sales\")\n",
        ")\n",
        "\n",
        "\n",
        "quarterly_sales_rep=quarterly_sales_rep.orderBy(\"Quarter\")\n",
        "quarterly_sales_rep.show(truncate=False)\n",
        "quarterly_sales_rep.write.csv(\"/content/sample_data/quarterly_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWEWNK8FBnIN",
        "outputId": "1e1770ca-42c9-4425-e828-feef30892c94"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|Quarter|Quarterly_Sales|\n",
            "+-------+---------------+\n",
            "|Q1     |1708000        |\n",
            "+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.For each quartetr report for each year Write udf function for that.\n",
        "And for all year**\n"
      ],
      "metadata": {
        "id": "0kqux1JAxB5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col, sum\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "\n",
        "def get_quarter(month):\n",
        "    if 1 <= month <= 3:\n",
        "        return \"Q1\"\n",
        "    elif 4 <= month <= 6:\n",
        "        return \"Q2\"\n",
        "    elif 7 <= month <= 9:\n",
        "        return \"Q3\"\n",
        "    elif 10 <= month <= 12:\n",
        "        return \"Q4\"\n",
        "    else:\n",
        "        return \"Invalid\"\n",
        "\n",
        "quarter_udf = udf(get_quarter, StringType())\n",
        "\n",
        "df_with_quarter = df_with_fiscal.withColumn(\"Quarter\", quarter_udf(col(\"Month\")))\n",
        "\n",
        "quarterly_sales_all_years = df_with_quarter.groupBy(\"FiscalYear\", \"Quarter\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"Quarterly_Sales\")) \\\n",
        "    .orderBy(\"FiscalYear\", \"Quarter\")\n",
        "\n",
        "\n",
        "quarterly_sales_all_years.show(truncate=False)\n",
        "quarterly_sales_all_years.write.csv(\"/content/sample_data/quarterly_sales_all_years.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_4ALWGEB4-m",
        "outputId": "ef3e5833-a5f3-4096-c173-386ad0f3cf5f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------------+\n",
            "|FiscalYear|Quarter|Quarterly_Sales|\n",
            "+----------+-------+---------------+\n",
            "|2004      |Q1     |1180000        |\n",
            "|2005      |Q1     |1700000        |\n",
            "|2006      |Q1     |1708000        |\n",
            "+----------+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**half yearly sales rep of\t\"\t\"\n",
        "Report is dumped into all_hy_sales_rep**\n"
      ],
      "metadata": {
        "id": "YncZhTlcJS53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, sum, month, year, to_date\n",
        "\n",
        "df_with_date_parts = df.withColumn(\"DateParsed\", to_date(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Year\", year(col(\"DateParsed\")))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Month\", month(col(\"DateParsed\")))\n",
        "\n",
        "\n",
        "df_with_halfyear = df_with_date_parts.withColumn(\n",
        "    \"HalfYear\",\n",
        "    when(col(\"Month\") <= 6, \"H1\").otherwise(\"H2\")\n",
        ")\n",
        "\n",
        "all_hy_sales_rep = df_with_halfyear.groupBy(\"Year\", \"HalfYear\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"HalfYearly_Sales\")) \\\n",
        "    .orderBy(\"Year\", \"HalfYear\")\n",
        "\n",
        "all_hy_sales_rep.show(truncate=False)\n",
        "all_hy_sales_rep.write.csv(\"/content/sample_data/all_hy_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LirPM7icCT5X",
        "outputId": "7e8f4d39-b221-4924-9917-298cac1e6924"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+----------------+\n",
            "|Year|HalfYear|HalfYearly_Sales|\n",
            "+----+--------+----------------+\n",
            "|2005|H1      |1180000         |\n",
            "|2006|H1      |3408000         |\n",
            "+----+--------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**foreach year, monthly sales rep**"
      ],
      "metadata": {
        "id": "uiVGJcohJcr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col, month, year, to_date\n",
        "\n",
        "\n",
        "df_with_date_parts = df.withColumn(\"DateParsed\", to_date(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Year\", year(col(\"DateParsed\")))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Month\", month(col(\"DateParsed\")))\n",
        "\n",
        "\n",
        "monthly_sales_rep = df_with_date_parts.groupBy(\"Year\", \"Month\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"Monthly_Sales\")) \\\n",
        "    .orderBy(\"Year\", \"Month\")\n",
        "\n",
        "\n",
        "monthly_sales_rep.show(truncate=False)\n",
        "monthly_sales_rep.write.csv(\"/content/sample_data/monthly_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDxbxweMCxkv",
        "outputId": "7099571c-6d12-46be-d686-5aa5a2897701"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-------------+\n",
            "|Year|Month|Monthly_Sales|\n",
            "+----+-----+-------------+\n",
            "|2005|1    |730000       |\n",
            "|2005|2    |450000       |\n",
            "|2006|1    |2100000      |\n",
            "|2006|2    |1308000      |\n",
            "+----+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. foreach year, quarterly sales rep Report is dumped into all_qrt_sales_rep**"
      ],
      "metadata": {
        "id": "iy0w2S-JJnY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col, sum, month, year, to_date\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "\n",
        "def get_quarter(month):\n",
        "    if month in [1, 2, 3]:\n",
        "        return \"Q1\"\n",
        "    elif month in [4, 5, 6]:\n",
        "        return \"Q2\"\n",
        "    elif month in [7, 8, 9]:\n",
        "        return \"Q3\"\n",
        "    else:\n",
        "        return \"Q4\"\n",
        "\n",
        "\n",
        "quarter_udf = udf(get_quarter, StringType())\n",
        "\n",
        "df_with_date_parts = df.withColumn(\"DateParsed\", to_date(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Year\", year(col(\"DateParsed\")))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Month\", month(col(\"DateParsed\")))\n",
        "\n",
        "df_with_quarter = df_with_date_parts.withColumn(\"Quarter\", quarter_udf(col(\"Month\")))\n",
        "\n",
        "all_qrt_sales_rep = df_with_quarter.groupBy(\"Year\", \"Quarter\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"Quarterly_Sales\")) \\\n",
        "    .orderBy(\"Year\", \"Quarter\")\n",
        "\n",
        "all_qrt_sales_rep.show(truncate=False)\n",
        "all_qrt_sales_rep.write.csv(\"/content/sample_data/all_qrt_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MvUM5f2GNWt",
        "outputId": "e00204f0-849a-4222-ad0e-4f843bbd9065"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+---------------+\n",
            "|Year|Quarter|Quarterly_Sales|\n",
            "+----+-------+---------------+\n",
            "|2005|Q1     |1180000        |\n",
            "|2006|Q1     |3408000        |\n",
            "+----+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.foreACH year, half yearl sales rep. Report is dumped into fore_yr_sales_rep**"
      ],
      "metadata": {
        "id": "ZsyhPQjfKAG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf, col, sum, month, year, to_date\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def get_half_year(month):\n",
        "    return \"H1\" if month <= 6 else \"H2\"\n",
        "\n",
        "\n",
        "half_year_udf = udf(get_half_year, StringType())\n",
        "\n",
        "df_with_date_parts = df.withColumn(\"DateParsed\", to_date(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Year\", year(col(\"DateParsed\")))\n",
        "df_with_date_parts = df_with_date_parts.withColumn(\"Month\", month(col(\"DateParsed\")))\n",
        "\n",
        "\n",
        "df_with_half_year = df_with_date_parts.withColumn(\"HalfYear\", half_year_udf(col(\"Month\")))\n",
        "\n",
        "fore_yr_sales_rep = df_with_half_year.groupBy(\"Year\", \"HalfYear\") \\\n",
        "    .agg(sum(\"Amount\").alias(\"HalfYearly_Sales\")) \\\n",
        "    .orderBy(\"Year\", \"HalfYear\")\n",
        "\n",
        "fore_yr_sales_rep.show(truncate=False)\n",
        "fore_yr_sales_rep.write.csv(\"/content/sample_data/fore_yr_sales_rep.csv\",header=True,mode=\"overwrite\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJZFqkBOGNTS",
        "outputId": "bd057f5b-bd97-4bc9-cfa7-80acd19fb729"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+----------------+\n",
            "|Year|HalfYear|HalfYearly_Sales|\n",
            "+----+--------+----------------+\n",
            "|2005|H1      |1180000         |\n",
            "|2006|H1      |3408000         |\n",
            "+----+--------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. in a specific quart which product made more bussiness\n",
        "Compare all quarter according to product**"
      ],
      "metadata": {
        "id": "kIi8N24QKKbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/sample_data/retailsales.csv\")\n",
        "header = rdd.first()\n",
        "data_rdd = rdd.filter(lambda line: line != header)\n",
        "data_rdd.collect()\n",
        "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))\n",
        "parsed_rdd.collect()\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def map_to_quarter(row):\n",
        "    try:\n",
        "        date_str = row[0]\n",
        "        date_obj = datetime.strptime(date_str, \"%m/%d/%Y\")\n",
        "        quarter = (date_obj.month - 1) // 3 + 1\n",
        "        return (f\"Q{quarter}\", row)\n",
        "    except:\n",
        "        return (\"Invalid\", row)\n",
        "\n",
        "\n",
        "quarter_rdd = parsed_rdd.map(map_to_quarter)\n",
        "\n",
        "\n",
        "def get_amount(row):\n",
        "    try:\n",
        "        return int(row[1])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "\n",
        "quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
        "\n",
        "\n",
        "for q in quarters:\n",
        "    q_rdd = quarter_rdd.filter(lambda x: x[0] == q)\n",
        "\n",
        "    if not q_rdd.isEmpty():\n",
        "        max_row = q_rdd.map(lambda x: x[1]).max(key=get_amount)\n",
        "        print(f\"Top selling product in {q}: {max_row}\")\n",
        "    else:\n",
        "        print(f\"Top selling product in {q}: No data available\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS7q3R1ZHLh1",
        "outputId": "09460f6e-f177-4c99-8148-5be0a00bd0f4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top selling product in Q1: ['02/12/2006', '658000', 'China', 'Rum']\n",
            "Top selling product in Q2: No data available\n",
            "Top selling product in Q3: No data available\n",
            "Top selling product in Q4: No data available\n"
          ]
        }
      ]
    }
  ]
}