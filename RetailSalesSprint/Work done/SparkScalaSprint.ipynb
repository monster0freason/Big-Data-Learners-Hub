{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "spylon-kernel",
      "display_name": "spylon-kernel"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZF3hsVSpZDK",
        "outputId": "5191c726-3c6f-4bea-de06-42680922bbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for spylon-kernel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spylon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
            "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
            "0.00s - to python to disable frozen modules.\n",
            "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
            "[InstallKernelSpec] Installed kernelspec spylon-kernel in /usr/local/share/jupyter/kernels/spylon-kernel\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark spylon-kernel\n",
        "!python3 -m spylon_kernel install\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.SparkSession\n",
        "val spark= SparkSession.builder.appName(\"Spark with Scala Example\").master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "dtb8N7xUqAV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a119d3-43ea-4d09-83a4-974a81426def"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "import org.apache.spark.sql.SparkSession\n",
              "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@12cf9c99\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkScala For 5000 Sales Record**"
      ],
      "metadata": {
        "id": "SCxWfZe0uUwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val csvDF =spark.read.option(\"header\",\"true\").csv(\"/content/sample_data/5000 Sales Records.csv\")\n",
        "csvDF.show()\n",
        "csvDF.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOoFP_8eqfZU",
        "outputId": "da50d59c-2b18-4753-b84b-9bcc150c9a25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
            "|              Region|             Country|      Item Type|Sales Channel|Order Priority|Order Date| Order ID| Ship Date|Units Sold|Unit Price|Unit Cost|Total Revenue|Total Cost|Total Profit|\n",
            "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
            "|Central America a...|Antigua and Barbuda |      Baby Food|       Online|             M|12/20/2013|957081544| 1/11/2014|       552|    255.28|   159.42|    140914.56|  87999.84|    52914.72|\n",
            "|Central America a...|              Panama|         Snacks|      Offline|             C|  7/5/2010|301644504| 7/26/2010|      2167|    152.58|    97.44|    330640.86| 211152.48|   119488.38|\n",
            "|              Europe|      Czech Republic|      Beverages|      Offline|             C| 9/12/2011|478051030| 9/29/2011|      4778|     47.45|    31.79|    226716.10| 151892.62|    74823.48|\n",
            "|                Asia|         North Korea|         Cereal|      Offline|             L| 5/13/2010|892599952| 6/15/2010|      9016|    205.70|   117.11|   1854591.20|1055863.76|   798727.44|\n",
            "|                Asia|           Sri Lanka|         Snacks|      Offline|             C| 7/20/2015|571902596| 7/27/2015|      7542|    152.58|    97.44|   1150758.36| 734892.48|   415865.88|\n",
            "|Middle East and N...|             Morocco|  Personal Care|      Offline|             L| 11/8/2010|412882792|11/22/2010|        48|     81.73|    56.67|      3923.04|   2720.16|     1202.88|\n",
            "|Australia and Oce...|Federated States ...|        Clothes|      Offline|             H| 3/28/2011|932776868| 5/10/2011|      8258|    109.28|    35.84|    902434.24| 295966.72|   606467.52|\n",
            "|              Europe|Bosnia and Herzeg...|        Clothes|       Online|             M|10/14/2013|919133651| 11/4/2013|       927|    109.28|    35.84|    101302.56|  33223.68|    68078.88|\n",
            "|Middle East and N...|         Afghanistan|        Clothes|      Offline|             M| 8/27/2016|579814469| 10/5/2016|      8841|    109.28|    35.84|    966144.48| 316861.44|   649283.04|\n",
            "|  Sub-Saharan Africa|            Ethiopia|      Baby Food|       Online|             M| 4/13/2015|192993152|  5/7/2015|      9817|    255.28|   159.42|   2506083.76|1565026.14|   941057.62|\n",
            "|Middle East and N...|              Turkey|Office Supplies|      Offline|             C| 9/25/2013|557156026|10/15/2013|      3704|    651.21|   524.96|   2412081.84|1944451.84|   467630.00|\n",
            "|Middle East and N...|                Oman|      Cosmetics|       Online|             M| 5/12/2013|741101920| 5/17/2013|      7382|    437.20|   263.33|   3227410.40|1943902.06|  1283508.34|\n",
            "|                Asia|            Malaysia|         Cereal|      Offline|             L| 7/31/2016|333942162| 8/25/2016|      9762|    205.70|   117.11|   2008043.40|1143227.82|   864815.58|\n",
            "|Central America a...|         Saint Lucia|      Cosmetics|      Offline|             H|  7/6/2015|795100581| 7/16/2015|      6786|    437.20|   263.33|   2966839.20|1786957.38|  1179881.82|\n",
            "|Central America a...|Saint Vincent and...|      Baby Food|       Online|             L|11/28/2010|504313504| 12/3/2010|      6428|    255.28|   159.42|   1640939.84|1024751.76|   616188.08|\n",
            "|Middle East and N...|             Lebanon|           Meat|      Offline|             H|12/17/2015|611629760| 1/31/2016|      3693|    421.89|   364.69|   1558039.77|1346800.17|   211239.60|\n",
            "|              Europe|             Austria|         Cereal|      Offline|             C| 8/13/2014|987410676|  9/6/2014|      5616|    205.70|   117.11|   1155211.20| 657689.76|   497521.44|\n",
            "|              Europe|            Bulgaria|Office Supplies|       Online|             L|10/31/2010|672330081|11/29/2010|      6266|    651.21|   524.96|   4080481.86|3289399.36|   791082.50|\n",
            "|       North America|              Mexico|      Beverages|       Online|             C| 3/13/2017|127374303| 3/20/2017|      1742|     47.45|    31.79|     82657.90|  55378.18|    27279.72|\n",
            "|Central America a...| Trinidad and Tobago|      Baby Food|      Offline|             C| 4/16/2013|783842170|  6/1/2013|      5172|    255.28|   159.42|   1320308.16| 824520.24|   495787.92|\n",
            "+--------------------+--------------------+---------------+-------------+--------------+----------+---------+----------+----------+----------+---------+-------------+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "csvDF: org.apache.spark.sql.DataFrame = [Region: string, Country: string ... 12 more fields]\n",
              "res1: csvDF.type = [Region: string, Country: string ... 12 more fields]\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csvDF.printSchema()"
      ],
      "metadata": {
        "id": "iGapcDz_rYlI",
        "outputId": "f1369326-0dd8-4f5c-98dd-4b603b8ed462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Region: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- Item Type: string (nullable = true)\n",
            " |-- Sales Channel: string (nullable = true)\n",
            " |-- Order Priority: string (nullable = true)\n",
            " |-- Order Date: string (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Ship Date: string (nullable = true)\n",
            " |-- Units Sold: string (nullable = true)\n",
            " |-- Unit Price: string (nullable = true)\n",
            " |-- Unit Cost: string (nullable = true)\n",
            " |-- Total Revenue: string (nullable = true)\n",
            " |-- Total Cost: string (nullable = true)\n",
            " |-- Total Profit: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Display the products with atleast 2 occurences of 'a'.**"
      ],
      "metadata": {
        "id": "pTqD13fvr9oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val reducedDF=csvDF.select(\"Item Type\")\n",
        "val productsWithTwoOrMoreA = reducedDF.filter(\n",
        "  length(lower(col(\"Item Type\"))) - length(regexp_replace(lower(col(\"Item Type\")), \"a\", \"\")) >= 2\n",
        ")\n",
        "productsWithTwoOrMoreA.distinct().show()\n"
      ],
      "metadata": {
        "id": "A4zgsrGwrTj6",
        "outputId": "5bc58c1f-59fa-46d1-ede6-a8f76e5c24b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|    Item Type|\n",
            "+-------------+\n",
            "|Personal Care|\n",
            "+-------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reducedDF: org.apache.spark.sql.DataFrame = [Item Type: string]\n",
              "productsWithTwoOrMoreA: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Item Type: string]\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Display country in each region with highest units sold.**\n"
      ],
      "metadata": {
        "id": "YYAonRyhsSha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.expressions.Window\n",
        "\n",
        "// Start Spark session\n",
        "val spark = SparkSession.builder()\n",
        "  .appName(\"Spark DataFrame Analysis\")\n",
        "  .master(\"local[*]\")\n",
        "  .getOrCreate()\n",
        "\n",
        "import spark.implicits._\n",
        "\n",
        "val df = csvDF.select(\"Region\", \"Country\", \"Units Sold\")\n",
        "  .withColumn(\"Units Sold\", $\"Units Sold\".cast(\"int\"))\n",
        "\n",
        "// CASE 1: WITHOUT PARTITIONING\n",
        "\n",
        "val start1 = System.nanoTime()\n",
        "\n",
        "val windowSpec1 = Window.partitionBy(\"Region\").orderBy(desc(\"Units Sold\"))\n",
        "\n",
        "val topWithoutPartition = df\n",
        "  .withColumn(\"row_num\", row_number().over(windowSpec1))\n",
        "  .filter($\"row_num\" === 1)\n",
        "  .drop(\"row_num\")\n",
        "\n",
        "topWithoutPartition.show(false)\n",
        "\n",
        "val end1 = System.nanoTime()\n",
        "val duration1 = (end1 - start1) / 1e9d\n",
        "println(s\"\\nâ±ï¸ Time without partitioning: $duration1 seconds\")\n",
        "println(\"==================================================================\")\n",
        "\n",
        "// CASE 2: WITH PARTITIONING BY REGION\n",
        "\n",
        "val start2 = System.nanoTime()\n",
        "\n",
        "val repartitionedDF = df.repartition(col(\"Region\"))\n",
        "\n",
        "val windowSpec2 = Window.partitionBy(\"Region\").orderBy(desc(\"Units Sold\"))\n",
        "\n",
        "val topWithPartition = repartitionedDF\n",
        "  .withColumn(\"row_num\", row_number().over(windowSpec2))\n",
        "  .filter($\"row_num\" === 1)\n",
        "  .drop(\"row_num\")\n",
        "\n",
        "topWithPartition.show(false)\n",
        "\n",
        "val end2 = System.nanoTime()\n",
        "val duration2 = (end2 - start2) / 1e9d\n",
        "println(s\"\\nðŸš€ Time with partitioning: $duration2 seconds\")\n"
      ],
      "metadata": {
        "id": "ggSAcc9Kr4cS",
        "outputId": "b47b07ab-d5c1-4d83-fd0a-f39d23406bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+-----------------+----------+\n",
            "|Region                           |Country          |Units Sold|\n",
            "+---------------------------------+-----------------+----------+\n",
            "|Asia                             |Thailand         |9991      |\n",
            "|Australia and Oceania            |Samoa            |9990      |\n",
            "|Central America and the Caribbean|Dominica         |9996      |\n",
            "|Europe                           |United Kingdom   |9999      |\n",
            "|Middle East and North Africa     |Libya            |9881      |\n",
            "|North America                    |Greenland        |9898      |\n",
            "|Sub-Saharan Africa               |Equatorial Guinea|9984      |\n",
            "+---------------------------------+-----------------+----------+\n",
            "\n",
            "\n",
            "â±ï¸ Time without partitioning: 1.116491543 seconds\n",
            "==================================================================\n",
            "+---------------------------------+-----------------+----------+\n",
            "|Region                           |Country          |Units Sold|\n",
            "+---------------------------------+-----------------+----------+\n",
            "|Asia                             |Thailand         |9991      |\n",
            "|Australia and Oceania            |Samoa            |9990      |\n",
            "|Central America and the Caribbean|Dominica         |9996      |\n",
            "|Europe                           |United Kingdom   |9999      |\n",
            "|Middle East and North Africa     |Libya            |9881      |\n",
            "|North America                    |Greenland        |9898      |\n",
            "|Sub-Saharan Africa               |Equatorial Guinea|9984      |\n",
            "+---------------------------------+-----------------+----------+\n",
            "\n",
            "\n",
            "ðŸš€ Time with partitioning: 0.741601659 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "import org.apache.spark.sql.SparkSession\n",
              "import org.apache.spark.sql.functions._\n",
              "import org.apache.spark.sql.expressions.Window\n",
              "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@12cf9c99\n",
              "import spark.implicits._\n",
              "df: org.apache.spark.sql.DataFrame = [Region: string, Country: string ... 1 more field]\n",
              "start1: Long = 1363162198487\n",
              "windowSpec1: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@9f9d074\n",
              "topWithoutPartition: org.apache.spark.sql.DataFrame = [Region: string, Country: string ... 1 more field]\n",
              "end1: Long = 1364278690030\n",
              "duration1: Double = 1.116491543\n",
              "start2: Long = 1364285626489\n",
              "repartitionedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Region: string, Country: string ... 1 more field]\n",
              "windowSpec2:...\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Display the unit price and unit cost of each item in ascending order.**"
      ],
      "metadata": {
        "id": "J8M13f_4ssNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val priceCostDF=csvDF.select(col(\"Item Type\"),col(\"Unit Price\"),col(\"Unit Cost\")).distinct().orderBy(col(\"Unit Price\"),col(\"Unit Cost\"))\n",
        "priceCostDF.show()"
      ],
      "metadata": {
        "id": "Hn09k4hQsqjo",
        "outputId": "b7959a3f-5000-491f-a176-da431a9f616b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+---------+\n",
            "|      Item Type|Unit Price|Unit Cost|\n",
            "+---------------+----------+---------+\n",
            "|        Clothes|    109.28|    35.84|\n",
            "|         Snacks|    152.58|    97.44|\n",
            "|     Vegetables|    154.06|    90.93|\n",
            "|         Cereal|    205.70|   117.11|\n",
            "|      Baby Food|    255.28|   159.42|\n",
            "|           Meat|    421.89|   364.69|\n",
            "|      Cosmetics|    437.20|   263.33|\n",
            "|      Beverages|     47.45|    31.79|\n",
            "|Office Supplies|    651.21|   524.96|\n",
            "|      Household|    668.27|   502.54|\n",
            "|  Personal Care|     81.73|    56.67|\n",
            "|         Fruits|      9.33|     6.92|\n",
            "+---------------+----------+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "priceCostDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Item Type: string, Unit Price: string ... 1 more field]\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkScala For XML-> csv Record**"
      ],
      "metadata": {
        "id": "ZV2ZjvEvuesr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "import spark.implicits._\n",
        "\n",
        "\n",
        "val retail_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/content/sample_data/retailsales.csv\")\n",
        "\n",
        "val retail_df2 = retail_df.withColumn(\"DateParsed\", to_timestamp(col(\"Sales_Date\"), \"MM/dd/yyyy\"))\n",
        "\n",
        "val retail_df3 = retail_df2.withColumn(\"Year\", year(col(\"DateParsed\"))).withColumn(\"Month\", month(col(\"DateParsed\"))).withColumn(\"Day\", dayofmonth(col(\"DateParsed\")))\n",
        "\n",
        "retail_df3.select(\"Sales_Date\",\"Amount\", \"Country\", \"Product\", \"Year\", \"Month\", \"Day\").show(false)"
      ],
      "metadata": {
        "id": "Wq9yLOagrTgg",
        "outputId": "b3431edd-9f83-4fcb-ef6c-e325a4b915c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------+-------+----+-----+---+\n",
            "|Sales_Date|Amount|Country|Product|Year|Month|Day|\n",
            "+----------+------+-------+-------+----+-----+---+\n",
            "|01/23/2005|350000|India  |bear   |2005|1    |23 |\n",
            "|01/27/2005|380000|India  |visky  |2005|1    |27 |\n",
            "|02/12/2005|450000|India  |Rum    |2005|2    |12 |\n",
            "|01/23/2006|500000|USA    |bear   |2006|1    |23 |\n",
            "|01/27/2006|550000|USA    |rum    |2006|1    |27 |\n",
            "|02/12/2006|650000|USA    |Visky  |2006|2    |12 |\n",
            "|01/23/2006|500000|China  |Beer   |2006|1    |23 |\n",
            "|01/27/2006|550000|China  |Visky  |2006|1    |27 |\n",
            "|02/12/2006|658000|China  |Rum    |2006|2    |12 |\n",
            "+----------+------+-------+-------+----+-----+---+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "import org.apache.spark.sql.functions._\n",
              "import spark.implicits._\n",
              "retail_df: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 2 more fields]\n",
              "retail_df2: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 3 more fields]\n",
              "retail_df3: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 6 more fields]\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.yearly sales report**"
      ],
      "metadata": {
        "id": "gDAEVGogvrxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val yr_sales_rep = retail_df3.groupBy(\"Year\").agg(sum(\"Amount\").alias(\"Yearly Sales\")).orderBy(\"Year\")\n",
        "yr_sales_rep.show()"
      ],
      "metadata": {
        "id": "uWxI2CQNrTdl",
        "outputId": "4837e409-4612-4327-98c6-c6b401a60b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+\n",
            "|Year|Yearly Sales|\n",
            "+----+------------+\n",
            "|2005|     1180000|\n",
            "|2006|     3408000|\n",
            "+----+------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "yr_sales_rep: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Yearly Sales: bigint]\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Yearly sum for all count**"
      ],
      "metadata": {
        "id": "1Qa3EVqNv0ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val yearly_sum1 = retail_df3.groupBy(\"Year\",\"Country\").agg(sum(\"Amount\").alias(\"Country Sales\")).orderBy(\"Country\")\n",
        "yearly_sum1.show()"
      ],
      "metadata": {
        "id": "4IqNUC3VvzbW",
        "outputId": "af8129f5-098b-4050-e0ca-408b72456d98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+-------------+\n",
            "|Year|Country|Country Sales|\n",
            "+----+-------+-------------+\n",
            "|2006|  China|      1708000|\n",
            "|2005|  India|      1180000|\n",
            "|2006|    USA|      1700000|\n",
            "+----+-------+-------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "yearly_sum1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Country: string ... 1 more field]\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.Yearly sum for specified country**"
      ],
      "metadata": {
        "id": "xqHk_LpEv_J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val country_china = retail_df3.filter(col(\"Country\") === \"China\").groupBy(\"Year\").agg(sum(\"Amount\").alias(\"China Sales\")).orderBy(\"Year\")\n",
        "country_china.show()\n",
        "country_china.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"/content/sample_data/country_china.csv\")"
      ],
      "metadata": {
        "id": "y_iRNuzbv_nM",
        "outputId": "1ed4204d-aa36-4383-ef4c-be2cd8a25a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+\n",
            "|Year|China Sales|\n",
            "+----+-----------+\n",
            "|2006|    1708000|\n",
            "+----+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "country_china: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, China Sales: bigint]\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. yearly report dumping in the yr_sales_rep. Meanwhile we can apply all aggregation function here.**\n",
        "\n"
      ],
      "metadata": {
        "id": "-x--enOEwF4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions._\n",
        "\n",
        "val yr_sales_rep = retail_df3\n",
        "  .groupBy(\"Year\")\n",
        "  .agg(\n",
        "    sum(\"Amount\").alias(\"Total_Sales\"),\n",
        "    avg(\"Amount\").alias(\"Average_Sales\"),\n",
        "    min(\"Amount\").alias(\"Min_Sales\"),\n",
        "    max(\"Amount\").alias(\"Max_Sales\"),\n",
        "    count(\"Amount\").alias(\"Transaction_Count\"),\n",
        "    stddev(\"Amount\").alias(\"StdDev_Sales\")\n",
        "  )\n",
        "  .orderBy(\"Year\")\n",
        "\n",
        "yr_sales_rep.show()\n"
      ],
      "metadata": {
        "id": "t0NwvYkowE-S",
        "outputId": "58a464ae-96d3-4690-93eb-8c8dda062784",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+-----------------+---------+---------+-----------------+------------------+\n",
            "|Year|Total_Sales|    Average_Sales|Min_Sales|Max_Sales|Transaction_Count|      StdDev_Sales|\n",
            "+----+-----------+-----------------+---------+---------+-----------------+------------------+\n",
            "|2005|    1180000|393333.3333333333|   350000|   450000|                3|51316.014394468846|\n",
            "|2006|    3408000|         568000.0|   500000|   658000|                6| 70313.58332498778|\n",
            "+----+-----------+-----------------+---------+---------+-----------------+------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "import org.apache.spark.sql.functions._\n",
              "yr_sales_rep: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Year: int, Total_Sales: bigint ... 5 more fields]\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.monthly sales report of a perticular year Dumping the data into mn_sales_rep**"
      ],
      "metadata": {
        "id": "WEhldQmWwQb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val mn_sales_rep=retail_df3.filter(col(\"Year\") === 2005).groupBy(\"Month\").agg(sum(\"Amount\").alias(\"montly_sales\")).orderBy(\"Month\")\n",
        "mn_sales_rep.show()\n",
        "mn_sales_rep.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"/content/sample_data/mn_sales_rep.csv\")"
      ],
      "metadata": {
        "id": "8HwQPWa7wPPO",
        "outputId": "1f9e216a-c513-40fd-ab41-5c48c5c562d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Month|montly_sales|\n",
            "+-----+------------+\n",
            "|    1|      730000|\n",
            "|    2|      450000|\n",
            "+-----+------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mn_sales_rep: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Month: int, montly_sales: bigint]\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.quarterly sales report of a particular year**"
      ],
      "metadata": {
        "id": "A-AF-FPiwRdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val quaterly_sales=retail_df3.withColumn(\"Quarter\",quarter(to_date(concat_ws(\"-\",col(\"Year\"),col(\"Month\"),lit(1)),\"yyyy-M-d\")))\n",
        "quaterly_sales.show()\n",
        "quaterly_sales.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"/content/sample_data/quaterly_sales.csv\")\n"
      ],
      "metadata": {
        "id": "0jj36SxmwRBJ",
        "outputId": "3c09fe7f-3bb6-4b94-b557-c5c7cacc0187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "|Sales_Date|Amount|Country|Product|         DateParsed|Year|Month|Day|Quarter|\n",
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "|01/23/2005|350000|  India|   bear|2005-01-23 00:00:00|2005|    1| 23|      1|\n",
            "|01/27/2005|380000|  India|  visky|2005-01-27 00:00:00|2005|    1| 27|      1|\n",
            "|02/12/2005|450000|  India|    Rum|2005-02-12 00:00:00|2005|    2| 12|      1|\n",
            "|01/23/2006|500000|    USA|   bear|2006-01-23 00:00:00|2006|    1| 23|      1|\n",
            "|01/27/2006|550000|    USA|    rum|2006-01-27 00:00:00|2006|    1| 27|      1|\n",
            "|02/12/2006|650000|    USA|  Visky|2006-02-12 00:00:00|2006|    2| 12|      1|\n",
            "|01/23/2006|500000|  China|   Beer|2006-01-23 00:00:00|2006|    1| 23|      1|\n",
            "|01/27/2006|550000|  China|  Visky|2006-01-27 00:00:00|2006|    1| 27|      1|\n",
            "|02/12/2006|658000|  China|    Rum|2006-02-12 00:00:00|2006|    2| 12|      1|\n",
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quaterly_sales: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 7 more fields]\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val quaterly_df = retail_df3.withColumn(\"Quarter\", quarter(col(\"DateParsed\")))\n",
        "\n",
        "val quaterly_year = quaterly_df.filter(col(\"Year\") === 2005).groupBy(\"Quarter\").agg(sum(\"Amount\").alias(\"Quarterly Sales\")).orderBy(\"Quarter\")\n",
        "quaterly_year.show()\n",
        "quaterly_year.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(\"/content/sample_data/quarterly_year.csv\")"
      ],
      "metadata": {
        "id": "421v8EM4wYd5",
        "outputId": "d75f54f4-e7af-4b0c-86fc-9d93400b68a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------+\n",
            "|Quarter|Quarterly Sales|\n",
            "+-------+---------------+\n",
            "|      1|        1180000|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "quaterly_df: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 7 more fields]\n",
              "quaterly_year: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Quarter: int, Quarterly Sales: bigint]\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.For each quartetr report for each year Write udf function for that.\n",
        "And for all year**\n"
      ],
      "metadata": {
        "id": "0kqux1JAxB5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.functions.udf\n",
        "val getQuarterUDF = udf((month: Int) => {\n",
        "  if (month >= 1 && month <= 3) 1: java.lang.Integer\n",
        "  else if (month >= 4 && month <= 6) 2: java.lang.Integer\n",
        "  else if (month >= 7 && month <= 9) 3: java.lang.Integer\n",
        "  else if (month >= 10 && month <= 12) 4: java.lang.Integer\n",
        "  else null.asInstanceOf[java.lang.Integer]\n",
        "})\n",
        "val withQuarter = retail_df3.withColumn(\"Quarter\", getQuarterUDF(col(\"Month\")))\n",
        "withQuarter.show()"
      ],
      "metadata": {
        "id": "oeXooe4_xA68",
        "outputId": "410666c9-e9c0-4778-be68-0be5c2d41292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "|Sales_Date|Amount|Country|Product|         DateParsed|Year|Month|Day|Quarter|\n",
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "|01/23/2005|350000|  India|   bear|2005-01-23 00:00:00|2005|    1| 23|      1|\n",
            "|01/27/2005|380000|  India|  visky|2005-01-27 00:00:00|2005|    1| 27|      1|\n",
            "|02/12/2005|450000|  India|    Rum|2005-02-12 00:00:00|2005|    2| 12|      1|\n",
            "|01/23/2006|500000|    USA|   bear|2006-01-23 00:00:00|2006|    1| 23|      1|\n",
            "|01/27/2006|550000|    USA|    rum|2006-01-27 00:00:00|2006|    1| 27|      1|\n",
            "|02/12/2006|650000|    USA|  Visky|2006-02-12 00:00:00|2006|    2| 12|      1|\n",
            "|01/23/2006|500000|  China|   Beer|2006-01-23 00:00:00|2006|    1| 23|      1|\n",
            "|01/27/2006|550000|  China|  Visky|2006-01-27 00:00:00|2006|    1| 27|      1|\n",
            "|02/12/2006|658000|  China|    Rum|2006-02-12 00:00:00|2006|    2| 12|      1|\n",
            "+----------+------+-------+-------+-------------------+----+-----+---+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "import org.apache.spark.sql.functions.udf\n",
              "getQuarterUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5481/0x0000000841c35040@4509bb9c,IntegerType,List(Some(class[value[0]: int])),Some(class[value[0]: int]),None,true,true)\n",
              "withQuarter: org.apache.spark.sql.DataFrame = [Sales_Date: string, Amount: int ... 7 more fields]\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}