{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326210dc-9f68-4675-85a5-def8478c2016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/25 09:39:46 INFO SparkContext: Running Spark version 3.5.1\n",
      "25/07/25 09:39:46 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "25/07/25 09:39:46 INFO SparkContext: Java version 11.0.26\n",
      "25/07/25 09:39:47 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "25/07/25 09:39:47 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 09:39:47 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/25 09:39:47 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 09:39:47 INFO SparkContext: Submitted application: Ques14\n",
      "25/07/25 09:39:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/25 09:39:47 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/25 09:39:47 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/25 09:39:47 INFO SecurityManager: Changing view acls to: dhsoni\n",
      "25/07/25 09:39:47 INFO SecurityManager: Changing modify acls to: dhsoni\n",
      "25/07/25 09:39:47 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/25 09:39:47 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/25 09:39:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dhsoni; groups with view permissions: EMPTY; users with modify permissions: dhsoni; groups with modify permissions: EMPTY\n",
      "25/07/25 09:39:50 INFO Utils: Successfully started service 'sparkDriver' on port 55765.\n",
      "25/07/25 09:39:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/25 09:39:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/25 09:39:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/25 09:39:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/25 09:39:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/25 09:39:50 INFO DiskBlockManager: Created local directory at C:\\Users\\dhsoni\\AppData\\Local\\Temp\\blockmgr-ea9e7f87-8d48-4eb4-88b5-b081c4442500\n",
      "25/07/25 09:39:50 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/25 09:39:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/25 09:39:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/25 09:39:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/25 09:39:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/07/25 09:39:51 INFO Executor: Starting executor ID driver on host 10.220.71.22\n",
      "25/07/25 09:39:51 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "25/07/25 09:39:51 INFO Executor: Java version 11.0.26\n",
      "25/07/25 09:39:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/25 09:39:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@794e93f4 for default.\n",
      "25/07/25 09:39:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55817.\n",
      "25/07/25 09:39:52 INFO NettyBlockTransferService: Server created on 10.220.71.22:55817\n",
      "25/07/25 09:39:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/25 09:39:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.220.71.22, 55817, None)\n",
      "25/07/25 09:39:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.220.71.22:55817 with 2.1 GiB RAM, BlockManagerId(driver, 10.220.71.22, 55817, None)\n",
      "25/07/25 09:39:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.220.71.22, 55817, None)\n",
      "25/07/25 09:39:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.220.71.22, 55817, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3fa3c2cc"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques14\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1baf5f4-3787-47e4-91b7-f9686770695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4227c5-ced7-49cc-8be5-47ccebeb23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 09:40:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/25 09:40:03 INFO SharedState: Warehouse path is 'file:/C:/Users/dhsoni/spark-warehouse'.\n",
      "25/07/25 09:40:06 INFO InMemoryFileIndex: It took 155 ms to list leaf files for 1 paths.\n",
      "25/07/25 09:40:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new)\n",
      "25/07/25 09:40:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#1),isnotnull(state_of_building#9),(type_of_property#1 = house),(state_of_building#9 = as new)\n",
      "25/07/25 09:40:14 INFO CodeGenerator: Code generated in 1421.3409 ms\n",
      "25/07/25 09:40:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.220.71.22:55817 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:14 INFO SparkContext: Created broadcast 0 from show at cmd3.sc:6\n",
      "25/07/25 09:40:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Registering RDD 3 (show at cmd3.sc:6) as input to shuffle 0\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Got map stage job 0 (show at cmd3.sc:6) with 1 output partitions\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cmd3.sc:6)\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:15 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:6), which has no missing parents\n",
      "25/07/25 09:40:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 39.5 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.220.71.22:55817 (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/25 09:40:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 112.2122 ms\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 35.3406 ms\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 17.7981 ms\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 21.2335 ms\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 25.1115 ms\n",
      "25/07/25 09:40:16 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/25 09:40:16 INFO CodeGenerator: Code generated in 19.6478 ms\n",
      "25/07/25 09:40:17 INFO CodeGenerator: Code generated in 15.4444 ms\n",
      "25/07/25 09:40:17 INFO CodeGenerator: Code generated in 18.4298 ms\n",
      "25/07/25 09:40:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2953 bytes result sent to driver\n",
      "25/07/25 09:40:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2004 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:18 INFO DAGScheduler: ShuffleMapStage 0 (show at cmd3.sc:6) finished in 2.255 s\n",
      "25/07/25 09:40:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/25 09:40:18 INFO DAGScheduler: running: HashSet()\n",
      "25/07/25 09:40:18 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/25 09:40:18 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/25 09:40:18 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/25 09:40:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/25 09:40:18 INFO CodeGenerator: Code generated in 49.4884 ms\n",
      "25/07/25 09:40:18 INFO SparkContext: Starting job: show at cmd3.sc:6\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Got job 1 (show at cmd3.sc:6) with 1 output partitions\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd3.sc:6)\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:6), which has no missing parents\n",
      "25/07/25 09:40:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.9 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.220.71.22:55817 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/25 09:40:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)\n",
      "25/07/25 09:40:18 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/25 09:40:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms\n",
      "25/07/25 09:40:18 INFO CodeGenerator: Code generated in 39.4016 ms\n",
      "25/07/25 09:40:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5772 bytes result sent to driver\n",
      "25/07/25 09:40:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 347 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:19 INFO DAGScheduler: ResultStage 2 (show at cmd3.sc:6) finished in 0.390 s\n",
      "25/07/25 09:40:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/25 09:40:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/25 09:40:19 INFO DAGScheduler: Job 1 finished: show at cmd3.sc:6, took 0.443912 s\n",
      "25/07/25 09:40:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.220.71.22:55817 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:22 INFO CodeGenerator: Code generated in 23.1112 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "| subtype_of_property|Total_price_New|\n",
      "+--------------------+---------------+\n",
      "|         manor house|        5944000|\n",
      "|               villa|      479928365|\n",
      "|           farmhouse|        6437500|\n",
      "|              castle|        1799999|\n",
      "|  mixed use building|       42838711|\n",
      "|     apartment block|       58532399|\n",
      "|              chalet|        3953300|\n",
      "|            bungalow|       12327000|\n",
      "|      other property|        5257650|\n",
      "|             mansion|       57171000|\n",
      "|     country cottage|       26633900|\n",
      "|               house|     1451322529|\n",
      "|          town house|       38780409|\n",
      "|exceptional property|       93186773|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mhouseNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint]\r\n",
       "\u001b[36mhouseRenovated\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceRen\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_Renovated: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "val houseNew = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"as new\")\n",
    "// houseNew.show()\n",
    "val totPriceNew = houseNew.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_New\"))\n",
    "totPriceNew.show()\n",
    "\n",
    "// For Renovated\n",
    "val houseRenovated = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"just renovated\")\n",
    "// houseRenovated.show()\n",
    "val totPriceRen = houseRenovated.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_Renovated\"))\n",
    "// totPriceRen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794f25e8-f5b9-4900-81cc-a68166f0bfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 09:40:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.220.71.22:55817 in memory (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new),IsNotNull(subtype_of_property)\n",
      "25/07/25 09:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#1),isnotnull(state_of_building#9),(type_of_property#1 = house),(state_of_building#9 = as new),isnotnull(subtype_of_property#2)\n",
      "25/07/25 09:40:27 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,just renovated),IsNotNull(subtype_of_property)\n",
      "25/07/25 09:40:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#71),isnotnull(state_of_building#79),(type_of_property#71 = house),(state_of_building#79 = just renovated),isnotnull(subtype_of_property#72)\n",
      "25/07/25 09:40:27 INFO CodeGenerator: Code generated in 83.5873 ms\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.220.71.22:55817 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO SparkContext: Created broadcast 3 from show at cmd4.sc:12\n",
      "25/07/25 09:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Registering RDD 10 (show at cmd4.sc:12) as input to shuffle 1\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Got map stage job 2 (show at cmd4.sc:12) with 1 output partitions\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at cmd4.sc:12)\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd4.sc:12), which has no missing parents\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.9 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.220.71.22:55817 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd4.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:27 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:27 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/25 09:40:27 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "25/07/25 09:40:27 INFO CodeGenerator: Code generated in 93.8031 ms\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO CodeGenerator: Code generated in 77.8424 ms\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.220.71.22:55817 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO SparkContext: Created broadcast 5 from show at cmd4.sc:12\n",
      "25/07/25 09:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/25 09:40:27 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Registering RDD 14 (show at cmd4.sc:12) as input to shuffle 2\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Got map stage job 3 (show at cmd4.sc:12) with 1 output partitions\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (show at cmd4.sc:12)\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[14] at show at cmd4.sc:12), which has no missing parents\n",
      "25/07/25 09:40:27 INFO CodeGenerator: Code generated in 13.6039 ms\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.1 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.220.71.22:55817 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:27 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[14] at show at cmd4.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:27 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:27 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/25 09:40:27 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "25/07/25 09:40:27 INFO CodeGenerator: Code generated in 74.7251 ms\n",
      "25/07/25 09:40:28 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/25 09:40:28 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2910 bytes result sent to driver\n",
      "25/07/25 09:40:28 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 713 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:28 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:28 INFO DAGScheduler: ShuffleMapStage 3 (show at cmd4.sc:12) finished in 0.751 s\n",
      "25/07/25 09:40:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/25 09:40:28 INFO DAGScheduler: running: HashSet(ShuffleMapStage 4)\n",
      "25/07/25 09:40:28 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/25 09:40:28 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/25 09:40:28 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2910 bytes result sent to driver\n",
      "25/07/25 09:40:28 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 536 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:28 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:28 INFO DAGScheduler: ShuffleMapStage 4 (show at cmd4.sc:12) finished in 0.567 s\n",
      "25/07/25 09:40:28 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/25 09:40:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/25 09:40:28 INFO DAGScheduler: running: HashSet()\n",
      "25/07/25 09:40:28 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/25 09:40:28 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/25 09:40:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/25 09:40:28 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.220.71.22:55817 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO CodeGenerator: Code generated in 31.9729 ms\n",
      "25/07/25 09:40:28 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "25/07/25 09:40:28 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 43.1 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.220.71.22:55817 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:28 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:28 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/25 09:40:28 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
      "25/07/25 09:40:28 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/25 09:40:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/25 09:40:28 INFO CodeGenerator: Code generated in 27.6081 ms\n",
      "25/07/25 09:40:28 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 5619 bytes result sent to driver\n",
      "25/07/25 09:40:28 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 92 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:28 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:28 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.114 s\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/25 09:40:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "25/07/25 09:40:28 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.132125 s\n",
      "25/07/25 09:40:28 INFO CodeGenerator: Code generated in 26.5144 ms\n",
      "25/07/25 09:40:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.0 MiB, free 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 750.0 B, free 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.220.71.22:55817 (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/25 09:40:28 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/25 09:40:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.220.71.22:55817 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/25 09:40:28 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.220.71.22:55817 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:28 INFO CodeGenerator: Code generated in 39.632 ms\n",
      "25/07/25 09:40:29 INFO SparkContext: Starting job: show at cmd4.sc:12\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Got job 5 (show at cmd4.sc:12) with 1 output partitions\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Final stage: ResultStage 8 (show at cmd4.sc:12)\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at show at cmd4.sc:12), which has no missing parents\n",
      "25/07/25 09:40:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 63.2 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:29 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.220.71.22:55817 (size: 26.2 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:29 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at show at cmd4.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/25 09:40:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 5)\n",
      "25/07/25 09:40:29 INFO ShuffleBlockFetcherIterator: Getting 1 (1143.0 B) non-empty blocks including 1 (1143.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/25 09:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/07/25 09:40:29 INFO CodeGenerator: Code generated in 40.5331 ms\n",
      "25/07/25 09:40:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 5). 8757 bytes result sent to driver\n",
      "25/07/25 09:40:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.220.71.22:55817 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 125 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:29 INFO DAGScheduler: ResultStage 8 (show at cmd4.sc:12) finished in 0.162 s\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/25 09:40:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "25/07/25 09:40:29 INFO DAGScheduler: Job 5 finished: show at cmd4.sc:12, took 0.190263 s\n",
      "25/07/25 09:40:29 INFO CodeGenerator: Code generated in 23.0033 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------------------+----------------+\n",
      "| subtype_of_property|Total_price_New|Total_price_Renovated|Price Difference|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "|         manor house|        5944000|              2499000|         3445000|\n",
      "|               villa|      479928365|             39234499|       440693866|\n",
      "|           farmhouse|        6437500|              2629000|         3808500|\n",
      "|  mixed use building|       42838711|             22061399|        20777312|\n",
      "|     apartment block|       58532399|             25896399|        32636000|\n",
      "|              chalet|        3953300|               284500|         3668800|\n",
      "|            bungalow|       12327000|              3535500|         8791500|\n",
      "|      other property|        5257650|              1328000|         3929650|\n",
      "|     country cottage|       26633900|             10623000|        16010900|\n",
      "|             mansion|       57171000|             15726900|        41444100|\n",
      "|               house|     1451322529|            357589108|      1093733421|\n",
      "|          town house|       38780409|              8910400|        29870009|\n",
      "|exceptional property|       93186773|             12040000|        81146773|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmerge2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 1 more field]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val merge2 = (totPriceNew\n",
    "             .join(totPriceRen, Seq(\"subtype_of_property\"))\n",
    "              .select(\n",
    "                  col(\"subtype_of_property\"),\n",
    "                  col(\"Total_price_New\"),\n",
    "                  col(\"Total_price_Renovated\")\n",
    "              )\n",
    "             )\n",
    "// merge2.show()\n",
    "\n",
    "val res = merge2.withColumn(\"Price Difference\", $\"Total_price_New\" - $\"Total_price_Renovated\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdf6c1c-b9c2-49b0-9bce-8a9dd6a3ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 09:42:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.220.71.22:55817 in memory (size: 19.4 KiB, free: 2.1 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved as orc"
     ]
    }
   ],
   "source": [
    "res.write.orc(\"Results/Res14.orc\")\n",
    "print(\"File Saved as orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d748e-e700-4865-8684-a2ca69dcf688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccb1f9-df44-489b-9621-b800bd23da51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
