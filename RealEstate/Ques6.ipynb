{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c231bac-318c-43c4-b1ea-5c80417ab46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/24 14:30:22 INFO SparkContext: Running Spark version 3.5.1\n",
      "25/07/24 14:30:22 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 14:30:22 INFO SparkContext: Java version 11.0.26\n",
      "25/07/24 14:30:23 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "25/07/24 14:30:23 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 14:30:23 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/24 14:30:23 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 14:30:23 INFO SparkContext: Submitted application: Analysis Ques 6\n",
      "25/07/24 14:30:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/24 14:30:23 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/24 14:30:23 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/24 14:30:23 INFO SecurityManager: Changing view acls to: dhsoni\n",
      "25/07/24 14:30:23 INFO SecurityManager: Changing modify acls to: dhsoni\n",
      "25/07/24 14:30:23 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/24 14:30:23 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/24 14:30:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dhsoni; groups with view permissions: EMPTY; users with modify permissions: dhsoni; groups with modify permissions: EMPTY\n",
      "25/07/24 14:30:25 INFO Utils: Successfully started service 'sparkDriver' on port 65087.\n",
      "25/07/24 14:30:25 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/24 14:30:25 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/24 14:30:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/24 14:30:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/24 14:30:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/24 14:30:25 INFO DiskBlockManager: Created local directory at C:\\Users\\dhsoni\\AppData\\Local\\Temp\\blockmgr-d0d5e495-1c5f-4bf6-9c8f-7b36610b76b1\n",
      "25/07/24 14:30:25 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/24 14:30:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/24 14:30:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/07/24 14:30:26 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/07/24 14:30:26 INFO Utils: Successfully started service 'SparkUI' on port 4046.\n",
      "25/07/24 14:30:26 INFO Executor: Starting executor ID driver on host 10.220.71.22\n",
      "25/07/24 14:30:26 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 14:30:26 INFO Executor: Java version 11.0.26\n",
      "25/07/24 14:30:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/24 14:30:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3e836904 for default.\n",
      "25/07/24 14:30:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65137.\n",
      "25/07/24 14:30:26 INFO NettyBlockTransferService: Server created on 10.220.71.22:65137\n",
      "25/07/24 14:30:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/24 14:30:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.220.71.22, 65137, None)\n",
      "25/07/24 14:30:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.220.71.22:65137 with 2.1 GiB RAM, BlockManagerId(driver, 10.220.71.22, 65137, None)\n",
      "25/07/24 14:30:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.220.71.22, 65137, None)\n",
      "25/07/24 14:30:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.220.71.22, 65137, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5a479915"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Analysis Ques 6\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3713a112-4da9-4b7e-856c-697a76fc8f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e20f9c7-ca66-41f5-9d08-aff8d6d18a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:33:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/24 14:33:26 INFO SharedState: Warehouse path is 'file:/C:/Users/dhsoni/spark-warehouse'.\n",
      "25/07/24 14:33:29 INFO InMemoryFileIndex: It took 151 ms to list leaf files for 1 paths.\n",
      "25/07/24 14:33:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/24 14:33:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#2),(subtype_of_property#2 = villa)\n",
      "25/07/24 14:33:39 INFO CodeGenerator: Code generated in 798.8719 ms\n",
      "25/07/24 14:33:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.220.71.22:65137 (size: 34.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:39 INFO SparkContext: Created broadcast 0 from show at cmd3.sc:6\n",
      "25/07/24 14:33:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Registering RDD 3 (show at cmd3.sc:6) as input to shuffle 0\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Got map stage job 0 (show at cmd3.sc:6) with 1 output partitions\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cmd3.sc:6)\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:6), which has no missing parents\n",
      "25/07/24 14:33:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 40.3 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.220.71.22:65137 (size: 18.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:33:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 75.4804 ms\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 24.4391 ms\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 15.9163 ms\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 13.3004 ms\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 12.1959 ms\n",
      "25/07/24 14:33:41 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 15.3868 ms\n",
      "25/07/24 14:33:41 INFO CodeGenerator: Code generated in 9.3945 ms\n",
      "25/07/24 14:33:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2953 bytes result sent to driver\n",
      "25/07/24 14:33:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2921 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:43 INFO DAGScheduler: ShuffleMapStage 0 (show at cmd3.sc:6) finished in 3.133 s\n",
      "25/07/24 14:33:43 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:33:43 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:33:43 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:33:43 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:33:44 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:33:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:33:44 INFO CodeGenerator: Code generated in 41.8024 ms\n",
      "25/07/24 14:33:44 INFO SparkContext: Starting job: show at cmd3.sc:6\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Got job 1 (show at cmd3.sc:6) with 1 output partitions\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd3.sc:6)\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:6), which has no missing parents\n",
      "25/07/24 14:33:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 43.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.220.71.22:65137 (size: 19.7 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:44 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:33:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)\n",
      "25/07/24 14:33:44 INFO ShuffleBlockFetcherIterator: Getting 1 (23.2 KiB) non-empty blocks including 1 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:33:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 29 ms\n",
      "25/07/24 14:33:44 INFO CodeGenerator: Code generated in 31.0872 ms\n",
      "25/07/24 14:33:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5697 bytes result sent to driver\n",
      "25/07/24 14:33:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 286 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:44 INFO DAGScheduler: ResultStage 2 (show at cmd3.sc:6) finished in 0.317 s\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:33:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/24 14:33:44 INFO DAGScheduler: Job 1 finished: show at cmd3.sc:6, took 0.366714 s\n",
      "25/07/24 14:33:44 INFO CodeGenerator: Code generated in 17.4096 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+\n",
      "|locality|Total_villas|Total_villas_price|\n",
      "+--------+------------+------------------+\n",
      "|    4900|           9|           5024000|\n",
      "|    9900|           6|           3308000|\n",
      "|    5300|           4|           1214000|\n",
      "|    1342|           3|           1495000|\n",
      "|    7340|          11|           3099000|\n",
      "|    7880|           1|            365000|\n",
      "|    2580|           4|           2042000|\n",
      "|    2235|           6|           3484500|\n",
      "|    1460|           3|           1249000|\n",
      "|    2387|           2|           1264000|\n",
      "|    4190|           1|            325000|\n",
      "|    2811|           1|            880000|\n",
      "|    7850|           8|           4508000|\n",
      "|    7387|           4|           1485000|\n",
      "|    1650|           8|           5339000|\n",
      "|    3000|           6|           3956000|\n",
      "|    7120|           6|           1416000|\n",
      "|    7530|           1|            259000|\n",
      "|    3220|           5|           2702000|\n",
      "|    4684|           1|            395000|\n",
      "+--------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:33:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.220.71.22:65137 in memory (size: 19.7 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:44 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/24 14:33:44 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#2),(subtype_of_property#2 = villa)\n",
      "25/07/24 14:33:44 INFO CodeGenerator: Code generated in 86.7299 ms\n",
      "25/07/24 14:33:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.220.71.22:65137 (size: 34.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO SparkContext: Created broadcast 3 from first at cmd3.sc:7\n",
      "25/07/24 14:33:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Registering RDD 10 (first at cmd3.sc:7) as input to shuffle 1\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Got map stage job 2 (first at cmd3.sc:7) with 1 output partitions\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (first at cmd3.sc:7)\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at first at cmd3.sc:7), which has no missing parents\n",
      "25/07/24 14:33:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 35.4 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.220.71.22:65137 (size: 16.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at first at cmd3.sc:7) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:45 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:45 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:33:45 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "25/07/24 14:33:45 INFO CodeGenerator: Code generated in 67.2621 ms\n",
      "25/07/24 14:33:45 INFO CodeGenerator: Code generated in 7.6849 ms\n",
      "25/07/24 14:33:45 INFO CodeGenerator: Code generated in 9.8538 ms\n",
      "25/07/24 14:33:45 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:33:45 INFO CodeGenerator: Code generated in 10.8831 ms\n",
      "25/07/24 14:33:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.220.71.22:65137 in memory (size: 34.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:45 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.220.71.22:65137 in memory (size: 18.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:46 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2910 bytes result sent to driver\n",
      "25/07/24 14:33:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 1861 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:46 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:46 INFO DAGScheduler: ShuffleMapStage 3 (first at cmd3.sc:7) finished in 1.877 s\n",
      "25/07/24 14:33:46 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:33:46 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:33:46 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:33:46 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:33:46 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:33:46 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:33:47 INFO CodeGenerator: Code generated in 22.7973 ms\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Registering RDD 13 (first at cmd3.sc:7) as input to shuffle 2\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Got map stage job 3 (first at cmd3.sc:7) with 1 output partitions\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (first at cmd3.sc:7)\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[13] at first at cmd3.sc:7), which has no missing parents\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 41.5 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.220.71.22:65137 (size: 18.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[13] at first at cmd3.sc:7) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:47 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7684 bytes) \n",
      "25/07/24 14:33:47 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
      "25/07/24 14:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 (20.3 KiB) non-empty blocks including 1 (20.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/07/24 14:33:47 INFO CodeGenerator: Code generated in 30.3507 ms\n",
      "25/07/24 14:33:47 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 5740 bytes result sent to driver\n",
      "25/07/24 14:33:47 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 122 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:47 INFO DAGScheduler: ShuffleMapStage 5 (first at cmd3.sc:7) finished in 0.145 s\n",
      "25/07/24 14:33:47 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:33:47 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:33:47 INFO CodeGenerator: Code generated in 14.3601 ms\n",
      "25/07/24 14:33:47 INFO SparkContext: Starting job: first at cmd3.sc:7\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Got job 4 (first at cmd3.sc:7) with 1 output partitions\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Final stage: ResultStage 8 (first at cmd3.sc:7)\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[16] at first at cmd3.sc:7), which has no missing parents\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.7 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.220.71.22:65137 (size: 6.3 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[16] at first at cmd3.sc:7) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:47 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:33:47 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.220.71.22:65137 in memory (size: 18.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:33:47 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/07/24 14:33:47 INFO CodeGenerator: Code generated in 16.9188 ms\n",
      "25/07/24 14:33:47 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 4127 bytes result sent to driver\n",
      "25/07/24 14:33:47 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 50 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:47 INFO DAGScheduler: ResultStage 8 (first at cmd3.sc:7) finished in 0.067 s\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Job 4 finished: first at cmd3.sc:7, took 0.079717 s\n",
      "25/07/24 14:33:47 INFO CodeGenerator: Code generated in 11.2959 ms\n",
      "25/07/24 14:33:47 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/24 14:33:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#2),(subtype_of_property#2 = villa)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.220.71.22:65137 in memory (size: 6.3 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.220.71.22:65137 (size: 34.1 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO SparkContext: Created broadcast 7 from show at cmd3.sc:11\n",
      "25/07/24 14:33:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Registering RDD 20 (show at cmd3.sc:11) as input to shuffle 3\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Got map stage job 5 (show at cmd3.sc:11) with 1 output partitions\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (show at cmd3.sc:11)\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[20] at show at cmd3.sc:11), which has no missing parents\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 40.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.220.71.22:65137 (size: 18.3 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:47 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[20] at show at cmd3.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:47 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:47 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:33:47 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)\n",
      "25/07/24 14:33:47 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:33:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.220.71.22:65137 in memory (size: 16.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:49 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 2910 bytes result sent to driver\n",
      "25/07/24 14:33:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 1716 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:49 INFO DAGScheduler: ShuffleMapStage 9 (show at cmd3.sc:11) finished in 1.740 s\n",
      "25/07/24 14:33:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:33:49 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:33:49 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:33:49 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:33:49 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:33:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:33:49 INFO CodeGenerator: Code generated in 26.2918 ms\n",
      "25/07/24 14:33:49 INFO SparkContext: Starting job: show at cmd3.sc:11\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Got job 6 (show at cmd3.sc:11) with 1 output partitions\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Final stage: ResultStage 11 (show at cmd3.sc:11)\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at show at cmd3.sc:11), which has no missing parents\n",
      "25/07/24 14:33:49 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 44.9 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:49 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.2 KiB, free 2.1 GiB)\n",
      "25/07/24 14:33:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.220.71.22:65137 (size: 20.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:33:49 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at show at cmd3.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:33:49 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:33:49 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 6) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:33:49 INFO Executor: Running task 0.0 in stage 11.0 (TID 6)\n",
      "25/07/24 14:33:49 INFO ShuffleBlockFetcherIterator: Getting 1 (23.2 KiB) non-empty blocks including 1 (23.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:33:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/24 14:33:49 INFO CodeGenerator: Code generated in 27.6587 ms\n",
      "25/07/24 14:33:49 INFO Executor: Finished task 0.0 in stage 11.0 (TID 6). 5317 bytes result sent to driver\n",
      "25/07/24 14:33:49 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 6) in 71 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:33:49 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:33:49 INFO DAGScheduler: ResultStage 11 (show at cmd3.sc:11) finished in 0.090 s\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:33:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "25/07/24 14:33:49 INFO DAGScheduler: Job 6 finished: show at cmd3.sc:11, took 0.105960 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+\n",
      "|locality|Total_villas|Total_villas_price|\n",
      "+--------+------------+------------------+\n",
      "|    1410|          64|          42087000|\n",
      "+--------+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mvillaDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mpriceVillas\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, Total_villas: bigint ... 1 more field]\r\n",
       "\u001b[36mmax_villa_count\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m64L\u001b[39m\r\n",
       "\u001b[36mres3_5\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m64L\u001b[39m\r\n",
       "\u001b[36mresult2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, Total_villas: bigint ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "\n",
    "val villaDf = haDF.filter($\"subtype_of_property\" === \"villa\")\n",
    "val priceVillas = villaDf.groupBy(\"locality\").agg(count(\"subtype_of_property\").alias(\"Total_villas\"), sum(\"price\").alias(\"Total_villas_price\"))\n",
    "priceVillas.show()\n",
    "val max_villa_count = priceVillas.agg(max(\"Total_villas\")).first().get(0)\n",
    "// max_villa_count\n",
    "\n",
    "val result2 = priceVillas.filter($\"Total_villas\" === max_villa_count)\n",
    "result2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
