{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c17f99-52fe-4870-b35d-d03f9434569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/25 09:39:39 INFO SparkContext: Running Spark version 3.5.1\n",
      "25/07/25 09:39:39 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "25/07/25 09:39:39 INFO SparkContext: Java version 11.0.26\n",
      "25/07/25 09:39:40 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "25/07/25 09:39:40 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 09:39:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/25 09:39:40 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 09:39:40 INFO SparkContext: Submitted application: Ques9\n",
      "25/07/25 09:39:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/25 09:39:40 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/25 09:39:40 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/25 09:39:40 INFO SecurityManager: Changing view acls to: dhsoni\n",
      "25/07/25 09:39:40 INFO SecurityManager: Changing modify acls to: dhsoni\n",
      "25/07/25 09:39:40 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/25 09:39:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/25 09:39:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dhsoni; groups with view permissions: EMPTY; users with modify permissions: dhsoni; groups with modify permissions: EMPTY\n",
      "25/07/25 09:39:43 INFO Utils: Successfully started service 'sparkDriver' on port 55680.\n",
      "25/07/25 09:39:43 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/25 09:39:43 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/25 09:39:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/25 09:39:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/25 09:39:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/25 09:39:43 INFO DiskBlockManager: Created local directory at C:\\Users\\dhsoni\\AppData\\Local\\Temp\\blockmgr-53fba7a2-d752-4bc1-87c4-c9919b2209bb\n",
      "25/07/25 09:39:43 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/25 09:39:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/25 09:39:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/25 09:39:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/07/25 09:39:45 INFO Executor: Starting executor ID driver on host 10.220.71.22\n",
      "25/07/25 09:39:45 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "25/07/25 09:39:45 INFO Executor: Java version 11.0.26\n",
      "25/07/25 09:39:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/25 09:39:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@445f2a4a for default.\n",
      "25/07/25 09:39:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55729.\n",
      "25/07/25 09:39:45 INFO NettyBlockTransferService: Server created on 10.220.71.22:55729\n",
      "25/07/25 09:39:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/25 09:39:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.220.71.22, 55729, None)\n",
      "25/07/25 09:39:45 INFO BlockManagerMasterEndpoint: Registering block manager 10.220.71.22:55729 with 2.1 GiB RAM, BlockManagerId(driver, 10.220.71.22, 55729, None)\n",
      "25/07/25 09:39:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.220.71.22, 55729, None)\n",
      "25/07/25 09:39:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.220.71.22, 55729, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@408a3c25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques9\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9302beb1-bc9a-42f0-b822-d1caa6131e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b076bcd-fb18-4de2-87c2-a836d1c7dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/25 09:39:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/25 09:39:57 INFO SharedState: Warehouse path is 'file:/C:/Users/dhsoni/spark-warehouse'.\n",
      "25/07/25 09:40:01 INFO InMemoryFileIndex: It took 186 ms to list leaf files for 1 paths.\n",
      "25/07/25 09:40:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/25 09:40:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#2),(subtype_of_property#2 = villa)\n",
      "25/07/25 09:40:09 INFO CodeGenerator: Code generated in 1154.1081 ms\n",
      "25/07/25 09:40:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.220.71.22:55729 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:09 INFO SparkContext: Created broadcast 0 from show at cmd3.sc:11\n",
      "25/07/25 09:40:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Registering RDD 3 (show at cmd3.sc:11) as input to shuffle 0\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Got map stage job 0 (show at cmd3.sc:11) with 1 output partitions\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cmd3.sc:11)\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:11), which has no missing parents\n",
      "25/07/25 09:40:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 39.8 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.220.71.22:55729 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd3.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/25 09:40:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/25 09:40:11 INFO CodeGenerator: Code generated in 124.9258 ms\n",
      "25/07/25 09:40:11 INFO CodeGenerator: Code generated in 35.089 ms\n",
      "25/07/25 09:40:11 INFO CodeGenerator: Code generated in 20.3469 ms\n",
      "25/07/25 09:40:11 INFO CodeGenerator: Code generated in 23.3636 ms\n",
      "25/07/25 09:40:11 INFO CodeGenerator: Code generated in 24.4122 ms\n",
      "25/07/25 09:40:12 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/25 09:40:12 INFO CodeGenerator: Code generated in 21.5856 ms\n",
      "25/07/25 09:40:12 INFO CodeGenerator: Code generated in 15.0117 ms\n",
      "25/07/25 09:40:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2953 bytes result sent to driver\n",
      "25/07/25 09:40:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1983 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:13 INFO DAGScheduler: ShuffleMapStage 0 (show at cmd3.sc:11) finished in 2.221 s\n",
      "25/07/25 09:40:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/25 09:40:13 INFO DAGScheduler: running: HashSet()\n",
      "25/07/25 09:40:13 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/25 09:40:13 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/25 09:40:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/25 09:40:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/25 09:40:13 INFO CodeGenerator: Code generated in 80.4241 ms\n",
      "25/07/25 09:40:13 INFO SparkContext: Starting job: show at cmd3.sc:11\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Got job 1 (show at cmd3.sc:11) with 1 output partitions\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd3.sc:11)\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:11), which has no missing parents\n",
      "25/07/25 09:40:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 45.3 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 2.1 GiB)\n",
      "25/07/25 09:40:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.220.71.22:55729 (size: 20.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/25 09:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cmd3.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/25 09:40:14 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/25 09:40:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/25 09:40:14 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)\n",
      "25/07/25 09:40:14 INFO ShuffleBlockFetcherIterator: Getting 1 (138.0 B) non-empty blocks including 1 (138.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/25 09:40:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 38 ms\n",
      "25/07/25 09:40:14 INFO CodeGenerator: Code generated in 93.7543 ms\n",
      "25/07/25 09:40:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5390 bytes result sent to driver\n",
      "25/07/25 09:40:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 572 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/25 09:40:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/25 09:40:14 INFO DAGScheduler: ResultStage 2 (show at cmd3.sc:11) finished in 0.658 s\n",
      "25/07/25 09:40:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/25 09:40:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/25 09:40:14 INFO DAGScheduler: Job 1 finished: show at cmd3.sc:11, took 0.914779 s\n",
      "25/07/25 09:40:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.220.71.22:55729 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:15 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.220.71.22:55729 in memory (size: 20.4 KiB, free: 2.1 GiB)\n",
      "25/07/25 09:40:18 INFO CodeGenerator: Code generated in 23.155 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|garden|Avg_Price|\n",
      "+------+---------+\n",
      "|   Yes| 507060.0|\n",
      "|    No| 486102.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mvillaDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mresDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: int, Avg_Price: double]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: string, Avg_Price: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "\n",
    "val villaDf = haDF.filter($\"subtype_of_property\" === \"villa\")\n",
    "// villaDf.show()\n",
    "\n",
    "val resDf = villaDf.groupBy(\"garden\").agg(round(avg(\"price\"),0).alias(\"Avg_Price\"))\n",
    "// resDf.show()\n",
    "\n",
    "val res = resDf.withColumn(\"garden\", when(col(\"garden\") === 1, \"Yes\").when(col(\"garden\") === 0, \"No\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32618a-8196-4bf4-8892-a5f3a0b10358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330581f4-6aa0-4af4-a6b1-2ee1d092a718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
